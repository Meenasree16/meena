{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenasree16/meena/blob/main/final_msme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxHznnxk61ay",
        "outputId": "e3ba2009-be65-410c-82d8-f8c2332d8e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "GNk4n-et7Jlt",
        "outputId": "c7429285-7bb1-4da8-815b-1787809427cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f621e5a6-7442-4197-b7cd-468c7980881d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f621e5a6-7442-4197-b7cd-468c7980881d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Total  Book - MSME - Grand Final - 07_10_2024.pdf to Total  Book - MSME - Grand Final - 07_10_2024 (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from each page of the PDF.\n",
        "    Returns a list of dicts: [{page_num, text}, ...]\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        text = doc[page_num].get_text()\n",
        "        pages.append({\n",
        "            \"page_num\": page_num + 1,\n",
        "            \"text\": text.strip()\n",
        "        })\n",
        "\n",
        "    return pages"
      ],
      "metadata": {
        "id": "nV5lGPpZ7OdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Confirm the uploaded filename\n",
        "print(os.listdir())\n",
        "\n",
        "# Replace with your exact file name\n",
        "pdf_path = \"Total  Book - MSME - Grand Final - 07_10_2024.pdf\"\n",
        "pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Optional: Preview first 500 characters\n",
        "print(\"Preview Page 1:\\n\", pages[0][\"text\"][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Z81fZ_7Tij",
        "outputId": "b80c4fe8-dc78-4678-c63f-78a9469f698e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'Total  Book - MSME - Grand Final - 07_10_2024.pdf', 'Total  Book - MSME - Grand Final - 07_10_2024 (1).pdf', 'sample_data']\n",
            "Preview Page 1:\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer from your fine-tuned TinyLLaMA model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vipplav/tinyllama-finetuned-faq\")"
      ],
      "metadata": {
        "id": "7aymHYPn7tq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all page texts into one big string\n",
        "full_text = \"\\n\".join([page[\"text\"] for page in pages])"
      ],
      "metadata": {
        "id": "OuBieJ247wKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_by_tokens(text, tokenizer, chunk_size=350, overlap=50):\n",
        "    \"\"\"\n",
        "    Splits input text into overlapping chunks of chunk_size tokens with overlap.\n",
        "    \"\"\"\n",
        "    # Tokenize entire text\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = start + chunk_size\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "\n",
        "        chunks.append({\n",
        "            \"chunk_id\": len(chunks),\n",
        "            \"text\": chunk_text\n",
        "        })\n",
        "\n",
        "        # Move start forward by (chunk_size - overlap)\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ZjG9GA5p7y8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = chunk_text_by_tokens(full_text, tokenizer, chunk_size=350, overlap=50)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(text_chunks)} chunks (~350 tokens each)\")\n",
        "print(\"üìÑ Example chunk:\\n\", text_chunks[0][\"text\"][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmiy1IXQ73Pb",
        "outputId": "26790783-3dfd-4893-c223-71720dd09d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (58711 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 196 chunks (~350 tokens each)\n",
            "üìÑ Example chunk:\n",
            " \n",
            "\n",
            "HANDBOOK OF GOVERNMENT \n",
            "SCHEMES FOR MSMEs\n",
            " \n",
            "The compendium of schemes for MSMEs has been made to eÔ¨Ä ectively communicate \n",
            "and create awareness about Government of Telangana and Government of India schemes \n",
            "for the development of thriving MSME ecosystem in the state. \n",
            " \n",
            "Government of Telangana has a comprehensive MSME Policy 2024 where in various \n",
            "policy level measures have been enshrined for MSMEs to reduce the cost of doing business, \n",
            "help MSMEs achieve scale and improve competitiveness. In o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_name = \"Vipplav/tinyllama-finetuned-faq\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()  # inference mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LysCKkN48My5",
        "outputId": "b2df5d5b-d3ca-44e7-b472-3d3453495f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from Vipplav/tinyllama-finetuned-faq led to unexpected keys not found in the model: model.layers.0.self_attn.q_proj.lora_A.default.weight, model.layers.0.self_attn.q_proj.lora_B.default.weight, model.layers.0.self_attn.v_proj.lora_A.default.weight, model.layers.0.self_attn.v_proj.lora_B.default.weight, model.layers.1.self_attn.q_proj.lora_A.default.weight, model.layers.1.self_attn.q_proj.lora_B.default.weight, model.layers.1.self_attn.v_proj.lora_A.default.weight, model.layers.1.self_attn.v_proj.lora_B.default.weight, model.layers.10.self_attn.q_proj.lora_A.default.weight, model.layers.10.self_attn.q_proj.lora_B.default.weight, model.layers.10.self_attn.v_proj.lora_A.default.weight, model.layers.10.self_attn.v_proj.lora_B.default.weight, model.layers.11.self_attn.q_proj.lora_A.default.weight, model.layers.11.self_attn.q_proj.lora_B.default.weight, model.layers.11.self_attn.v_proj.lora_A.default.weight, model.layers.11.self_attn.v_proj.lora_B.default.weight, model.layers.12.self_attn.q_proj.lora_A.default.weight, model.layers.12.self_attn.q_proj.lora_B.default.weight, model.layers.12.self_attn.v_proj.lora_A.default.weight, model.layers.12.self_attn.v_proj.lora_B.default.weight, model.layers.13.self_attn.q_proj.lora_A.default.weight, model.layers.13.self_attn.q_proj.lora_B.default.weight, model.layers.13.self_attn.v_proj.lora_A.default.weight, model.layers.13.self_attn.v_proj.lora_B.default.weight, model.layers.14.self_attn.q_proj.lora_A.default.weight, model.layers.14.self_attn.q_proj.lora_B.default.weight, model.layers.14.self_attn.v_proj.lora_A.default.weight, model.layers.14.self_attn.v_proj.lora_B.default.weight, model.layers.15.self_attn.q_proj.lora_A.default.weight, model.layers.15.self_attn.q_proj.lora_B.default.weight, model.layers.15.self_attn.v_proj.lora_A.default.weight, model.layers.15.self_attn.v_proj.lora_B.default.weight, model.layers.16.self_attn.q_proj.lora_A.default.weight, model.layers.16.self_attn.q_proj.lora_B.default.weight, model.layers.16.self_attn.v_proj.lora_A.default.weight, model.layers.16.self_attn.v_proj.lora_B.default.weight, model.layers.17.self_attn.q_proj.lora_A.default.weight, model.layers.17.self_attn.q_proj.lora_B.default.weight, model.layers.17.self_attn.v_proj.lora_A.default.weight, model.layers.17.self_attn.v_proj.lora_B.default.weight, model.layers.18.self_attn.q_proj.lora_A.default.weight, model.layers.18.self_attn.q_proj.lora_B.default.weight, model.layers.18.self_attn.v_proj.lora_A.default.weight, model.layers.18.self_attn.v_proj.lora_B.default.weight, model.layers.19.self_attn.q_proj.lora_A.default.weight, model.layers.19.self_attn.q_proj.lora_B.default.weight, model.layers.19.self_attn.v_proj.lora_A.default.weight, model.layers.19.self_attn.v_proj.lora_B.default.weight, model.layers.2.self_attn.q_proj.lora_A.default.weight, model.layers.2.self_attn.q_proj.lora_B.default.weight, model.layers.2.self_attn.v_proj.lora_A.default.weight, model.layers.2.self_attn.v_proj.lora_B.default.weight, model.layers.20.self_attn.q_proj.lora_A.default.weight, model.layers.20.self_attn.q_proj.lora_B.default.weight, model.layers.20.self_attn.v_proj.lora_A.default.weight, model.layers.20.self_attn.v_proj.lora_B.default.weight, model.layers.21.self_attn.q_proj.lora_A.default.weight, model.layers.21.self_attn.q_proj.lora_B.default.weight, model.layers.21.self_attn.v_proj.lora_A.default.weight, model.layers.21.self_attn.v_proj.lora_B.default.weight, model.layers.3.self_attn.q_proj.lora_A.default.weight, model.layers.3.self_attn.q_proj.lora_B.default.weight, model.layers.3.self_attn.v_proj.lora_A.default.weight, model.layers.3.self_attn.v_proj.lora_B.default.weight, model.layers.4.self_attn.q_proj.lora_A.default.weight, model.layers.4.self_attn.q_proj.lora_B.default.weight, model.layers.4.self_attn.v_proj.lora_A.default.weight, model.layers.4.self_attn.v_proj.lora_B.default.weight, model.layers.5.self_attn.q_proj.lora_A.default.weight, model.layers.5.self_attn.q_proj.lora_B.default.weight, model.layers.5.self_attn.v_proj.lora_A.default.weight, model.layers.5.self_attn.v_proj.lora_B.default.weight, model.layers.6.self_attn.q_proj.lora_A.default.weight, model.layers.6.self_attn.q_proj.lora_B.default.weight, model.layers.6.self_attn.v_proj.lora_A.default.weight, model.layers.6.self_attn.v_proj.lora_B.default.weight, model.layers.7.self_attn.q_proj.lora_A.default.weight, model.layers.7.self_attn.q_proj.lora_B.default.weight, model.layers.7.self_attn.v_proj.lora_A.default.weight, model.layers.7.self_attn.v_proj.lora_B.default.weight, model.layers.8.self_attn.q_proj.lora_A.default.weight, model.layers.8.self_attn.q_proj.lora_B.default.weight, model.layers.8.self_attn.v_proj.lora_A.default.weight, model.layers.8.self_attn.v_proj.lora_B.default.weight, model.layers.9.self_attn.q_proj.lora_A.default.weight, model.layers.9.self_attn.q_proj.lora_B.default.weight, model.layers.9.self_attn.v_proj.lora_A.default.weight, model.layers.9.self_attn.v_proj.lora_B.default.weight. Loading adapter weights from Vipplav/tinyllama-finetuned-faq led to missing keys in the model: layers.0.self_attn.q_proj.lora_A.default.weight, layers.0.self_attn.q_proj.lora_B.default.weight, layers.0.self_attn.v_proj.lora_A.default.weight, layers.0.self_attn.v_proj.lora_B.default.weight, layers.1.self_attn.q_proj.lora_A.default.weight, layers.1.self_attn.q_proj.lora_B.default.weight, layers.1.self_attn.v_proj.lora_A.default.weight, layers.1.self_attn.v_proj.lora_B.default.weight, layers.2.self_attn.q_proj.lora_A.default.weight, layers.2.self_attn.q_proj.lora_B.default.weight, layers.2.self_attn.v_proj.lora_A.default.weight, layers.2.self_attn.v_proj.lora_B.default.weight, layers.3.self_attn.q_proj.lora_A.default.weight, layers.3.self_attn.q_proj.lora_B.default.weight, layers.3.self_attn.v_proj.lora_A.default.weight, layers.3.self_attn.v_proj.lora_B.default.weight, layers.4.self_attn.q_proj.lora_A.default.weight, layers.4.self_attn.q_proj.lora_B.default.weight, layers.4.self_attn.v_proj.lora_A.default.weight, layers.4.self_attn.v_proj.lora_B.default.weight, layers.5.self_attn.q_proj.lora_A.default.weight, layers.5.self_attn.q_proj.lora_B.default.weight, layers.5.self_attn.v_proj.lora_A.default.weight, layers.5.self_attn.v_proj.lora_B.default.weight, layers.6.self_attn.q_proj.lora_A.default.weight, layers.6.self_attn.q_proj.lora_B.default.weight, layers.6.self_attn.v_proj.lora_A.default.weight, layers.6.self_attn.v_proj.lora_B.default.weight, layers.7.self_attn.q_proj.lora_A.default.weight, layers.7.self_attn.q_proj.lora_B.default.weight, layers.7.self_attn.v_proj.lora_A.default.weight, layers.7.self_attn.v_proj.lora_B.default.weight, layers.8.self_attn.q_proj.lora_A.default.weight, layers.8.self_attn.q_proj.lora_B.default.weight, layers.8.self_attn.v_proj.lora_A.default.weight, layers.8.self_attn.v_proj.lora_B.default.weight, layers.9.self_attn.q_proj.lora_A.default.weight, layers.9.self_attn.q_proj.lora_B.default.weight, layers.9.self_attn.v_proj.lora_A.default.weight, layers.9.self_attn.v_proj.lora_B.default.weight, layers.10.self_attn.q_proj.lora_A.default.weight, layers.10.self_attn.q_proj.lora_B.default.weight, layers.10.self_attn.v_proj.lora_A.default.weight, layers.10.self_attn.v_proj.lora_B.default.weight, layers.11.self_attn.q_proj.lora_A.default.weight, layers.11.self_attn.q_proj.lora_B.default.weight, layers.11.self_attn.v_proj.lora_A.default.weight, layers.11.self_attn.v_proj.lora_B.default.weight, layers.12.self_attn.q_proj.lora_A.default.weight, layers.12.self_attn.q_proj.lora_B.default.weight, layers.12.self_attn.v_proj.lora_A.default.weight, layers.12.self_attn.v_proj.lora_B.default.weight, layers.13.self_attn.q_proj.lora_A.default.weight, layers.13.self_attn.q_proj.lora_B.default.weight, layers.13.self_attn.v_proj.lora_A.default.weight, layers.13.self_attn.v_proj.lora_B.default.weight, layers.14.self_attn.q_proj.lora_A.default.weight, layers.14.self_attn.q_proj.lora_B.default.weight, layers.14.self_attn.v_proj.lora_A.default.weight, layers.14.self_attn.v_proj.lora_B.default.weight, layers.15.self_attn.q_proj.lora_A.default.weight, layers.15.self_attn.q_proj.lora_B.default.weight, layers.15.self_attn.v_proj.lora_A.default.weight, layers.15.self_attn.v_proj.lora_B.default.weight, layers.16.self_attn.q_proj.lora_A.default.weight, layers.16.self_attn.q_proj.lora_B.default.weight, layers.16.self_attn.v_proj.lora_A.default.weight, layers.16.self_attn.v_proj.lora_B.default.weight, layers.17.self_attn.q_proj.lora_A.default.weight, layers.17.self_attn.q_proj.lora_B.default.weight, layers.17.self_attn.v_proj.lora_A.default.weight, layers.17.self_attn.v_proj.lora_B.default.weight, layers.18.self_attn.q_proj.lora_A.default.weight, layers.18.self_attn.q_proj.lora_B.default.weight, layers.18.self_attn.v_proj.lora_A.default.weight, layers.18.self_attn.v_proj.lora_B.default.weight, layers.19.self_attn.q_proj.lora_A.default.weight, layers.19.self_attn.q_proj.lora_B.default.weight, layers.19.self_attn.v_proj.lora_A.default.weight, layers.19.self_attn.v_proj.lora_B.default.weight, layers.20.self_attn.q_proj.lora_A.default.weight, layers.20.self_attn.q_proj.lora_B.default.weight, layers.20.self_attn.v_proj.lora_A.default.weight, layers.20.self_attn.v_proj.lora_B.default.weight, layers.21.self_attn.q_proj.lora_A.default.weight, layers.21.self_attn.q_proj.lora_B.default.weight, layers.21.self_attn.v_proj.lora_A.default.weight, layers.21.self_attn.v_proj.lora_B.default.weight\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaModel(\n",
              "  (embed_tokens): Embedding(32000, 2048)\n",
              "  (layers): ModuleList(\n",
              "    (0-21): 22 x LlamaDecoderLayer(\n",
              "      (self_attn): LlamaAttention(\n",
              "        (q_proj): lora.Linear(\n",
              "          (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "          (lora_magnitude_vector): ModuleDict()\n",
              "        )\n",
              "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "        (v_proj): lora.Linear(\n",
              "          (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=8, out_features=256, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "          (lora_magnitude_vector): ModuleDict()\n",
              "        )\n",
              "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "      )\n",
              "      (mlp): LlamaMLP(\n",
              "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    )\n",
              "  )\n",
              "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "  (rotary_emb): LlamaRotaryEmbedding()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    Applies mean pooling to the hidden states, excluding padding tokens.\n",
        "    \"\"\"\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "    sum_hidden = torch.sum(hidden_states * input_mask_expanded, dim=1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "    return sum_hidden / sum_mask"
      ],
      "metadata": {
        "id": "WwjLftCi8PbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def embed_chunks_with_llama(text_chunks, tokenizer, model):\n",
        "#     embedded_data = []\n",
        "\n",
        "#     for chunk in text_chunks:\n",
        "#         text = chunk[\"text\"]\n",
        "\n",
        "#         # Tokenize\n",
        "#         inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "#         # Use last hidden state\n",
        "#         last_hidden = outputs.last_hidden_state\n",
        "\n",
        "#         # Mean pooling\n",
        "#         embedding = mean_pooling(last_hidden, inputs['attention_mask']).squeeze().numpy()\n",
        "\n",
        "#         # Save with metadata\n",
        "#         embedded_data.append({\n",
        "#             \"chunk_id\": chunk[\"chunk_id\"],\n",
        "#             \"text\": chunk[\"text\"],\n",
        "#             \"embedding\": embedding\n",
        "#         })\n",
        "\n",
        "#     return embedded_data"
      ],
      "metadata": {
        "id": "OeLyDCs78TOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def mean_pooling(hidden_states, attention_mask):\n",
        "    \"\"\"\n",
        "    Applies mean pooling to the hidden states, excluding padding tokens.\n",
        "    \"\"\"\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "    sum_hidden = torch.sum(hidden_states * input_mask_expanded, dim=1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "    return sum_hidden / sum_mask\n",
        "\n",
        "def embed_chunks_with_llama_batch(text_chunks, tokenizer, model, batch_size=8):\n",
        "    \"\"\"\n",
        "    Embeds each chunk using TinyLLaMA in batches on GPU.\n",
        "    \"\"\"\n",
        "    model = model.to(\"cuda\")  # Move model to GPU\n",
        "    model.eval()\n",
        "\n",
        "    embedded_data = []\n",
        "\n",
        "    for i in range(0, len(text_chunks), batch_size):\n",
        "        batch = text_chunks[i:i + batch_size]\n",
        "        texts = [chunk[\"text\"] for chunk in batch]\n",
        "\n",
        "        # Tokenize in batch\n",
        "        inputs = tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "        # Forward pass without gradient\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "\n",
        "        # Mean pooling\n",
        "        pooled = mean_pooling(last_hidden, inputs[\"attention_mask\"])  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Store each embedding with its metadata\n",
        "        for j, chunk in enumerate(batch):\n",
        "            embedded_data.append({\n",
        "                \"chunk_id\": chunk[\"chunk_id\"],\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"embedding\": pooled[j].cpu().numpy()\n",
        "            })\n",
        "\n",
        "        print(f\"‚úÖ Embedded {i + len(batch)}/{len(text_chunks)} chunks\")\n",
        "\n",
        "    return embedded_data"
      ],
      "metadata": {
        "id": "F5w2Kwf19lnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Generate all embeddings\n",
        "embedded_data = embed_chunks_with_llama_batch(\n",
        "    text_chunks=text_chunks,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    batch_size=8  # Adjust this if memory issues occur\n",
        ")\n",
        "\n",
        "# üëÄ Preview output\n",
        "print(f\"\\n‚úÖ Total embeddings generated: {len(embedded_data)}\")\n",
        "\n",
        "print(\"üìå First chunk preview:\")\n",
        "print(\"üìù Text (first 300 chars):\\n\", embedded_data[0][\"text\"][:300])\n",
        "print(\"üìà Embedding vector (first 5 dims):\", embedded_data[0][\"embedding\"][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahvsn4T9-PyL",
        "outputId": "ad111a6f-5a36-4a66-8e1a-b4cf9d191551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embedded 8/196 chunks\n",
            "‚úÖ Embedded 16/196 chunks\n",
            "‚úÖ Embedded 24/196 chunks\n",
            "‚úÖ Embedded 32/196 chunks\n",
            "‚úÖ Embedded 40/196 chunks\n",
            "‚úÖ Embedded 48/196 chunks\n",
            "‚úÖ Embedded 56/196 chunks\n",
            "‚úÖ Embedded 64/196 chunks\n",
            "‚úÖ Embedded 72/196 chunks\n",
            "‚úÖ Embedded 80/196 chunks\n",
            "‚úÖ Embedded 88/196 chunks\n",
            "‚úÖ Embedded 96/196 chunks\n",
            "‚úÖ Embedded 104/196 chunks\n",
            "‚úÖ Embedded 112/196 chunks\n",
            "‚úÖ Embedded 120/196 chunks\n",
            "‚úÖ Embedded 128/196 chunks\n",
            "‚úÖ Embedded 136/196 chunks\n",
            "‚úÖ Embedded 144/196 chunks\n",
            "‚úÖ Embedded 152/196 chunks\n",
            "‚úÖ Embedded 160/196 chunks\n",
            "‚úÖ Embedded 168/196 chunks\n",
            "‚úÖ Embedded 176/196 chunks\n",
            "‚úÖ Embedded 184/196 chunks\n",
            "‚úÖ Embedded 192/196 chunks\n",
            "‚úÖ Embedded 196/196 chunks\n",
            "\n",
            "‚úÖ Total embeddings generated: 196\n",
            "üìå First chunk preview:\n",
            "üìù Text (first 300 chars):\n",
            " \n",
            "\n",
            "HANDBOOK OF GOVERNMENT \n",
            "SCHEMES FOR MSMEs\n",
            " \n",
            "The compendium of schemes for MSMEs has been made to eÔ¨Ä ectively communicate \n",
            "and create awareness about Government of Telangana and Government of India schemes \n",
            "for the development of thriving MSME ecosystem in the state. \n",
            " \n",
            "Government of Telangana has \n",
            "üìà Embedding vector (first 5 dims): [ 1.0254444   0.0342778  -0.2830852  -0.4766209   0.72036386]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6MiK7ez_Y-6",
        "outputId": "c1df8915-25fe-4003-ac1f-2bd4de2eefee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CtAkZAuu_jr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all embeddings to a numpy float32 matrix\n",
        "embedding_matrix = np.array([entry[\"embedding\"] for entry in embedded_data]).astype(\"float32\")\n",
        "\n",
        "# Normalize for cosine similarity (FAISS uses L2 distance)\n",
        "faiss.normalize_L2(embedding_matrix)\n",
        "\n",
        "# Create index\n",
        "index = faiss.IndexFlatIP(embedding_matrix.shape[1])  # IP = inner product (after normalization = cosine)\n",
        "index.add(embedding_matrix)\n",
        "\n",
        "print(f\"‚úÖ FAISS index built with {index.ntotal} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPUEqT2D_set",
        "outputId": "cd84e82a-2207-48d7-aae4-80f4e88ae639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index built with 196 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Vipplav/tinyllama-finetuned-faq\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")"
      ],
      "metadata": {
        "id": "VSrA1NzWBt4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(query, embedded_data, index, tokenizer, model, k=5):\n",
        "    \"\"\"\n",
        "    Encodes query using TinyLLaMA and retrieves top-k similar chunks using FAISS.\n",
        "    \"\"\"\n",
        "    # Tokenize and embed query using TinyLLaMA\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        last_hidden = outputs.hidden_states[-1]\n",
        "        query_embedding = mean_pooling(last_hidden, inputs[\"attention_mask\"]).cpu().numpy()\n",
        "\n",
        "    # Normalize and search\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Retrieve chunks\n",
        "    results = [embedded_data[i] for i in indices[0]]\n",
        "    return results"
      ],
      "metadata": {
        "id": "hl5rZ4QY_ubg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to apply for PMEGP scheme?\"\n",
        "top_chunks = retrieve_chunks(query, embedded_data, index, tokenizer, model, k=3)\n",
        "\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"\\nüîπ Rank {i+1} (Chunk ID {chunk['chunk_id']}):\")\n",
        "    print(chunk[\"text\"][:400])  # Print first 400 characters of the retrieved chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEyWTMUB_yLM",
        "outputId": "e9c6899d-c10d-48cc-dcc3-dd629ccfb4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Rank 1 (Chunk ID 79):\n",
            "thoroughly, and make sure that you fall under the \n",
            " \n",
            "eligibility criteria.\n",
            "‚Ä¢ \n",
            "In order to apply, you need to create a Login ID, which would be used in further \n",
            " \n",
            "communications.\n",
            "‚Ä¢ \n",
            "After you have created your user account, you can Login and Apply Online for the Scheme. \n",
            "Before proceeding to apply online, kindly make sure that you have read the Guidelines \n",
            "thoroughly and also checked the List of do\n",
            "\n",
            "üîπ Rank 2 (Chunk ID 3):\n",
            "IC)\n",
            "16-17\n",
            "15 Scheme for Promoting Innovation, Rural Industry & Entrepreneurship (ASPIRE)\n",
            "18\n",
            "16 KHADI GRAMODYOG VIKAS YOJANA - Umbrella Scheme\n",
            "19-20\n",
            "17 Revamped Scheme of Fund for Regeneration of Traditional Industries (SFURTI)\n",
            "21-22\n",
            "18 MSME SUSTAINABLE (ZED) CERTIFICATION\n",
            "23-24\n",
            "19 MSME - Innovative (Incubation, IPR and Design)\n",
            "25-26\n",
            "20 MSME Champions Scheme (Erstwhile CLCS-TUS)\n",
            "27\n",
            "21 Quality Manuf\n",
            "\n",
            "üîπ Rank 3 (Chunk ID 4):\n",
            "PS)\n",
            "32\n",
            "26 Scheme For Promotion of Manufacturing of Electronic Components and \n",
            "Semi-Conductors (Specs)\n",
            "33\n",
            "27 Technology Incubation and Development of Entrepreneurs (TIDE)\n",
            "34\n",
            "28 Support to International Patent Protection in Electronics and IT(SIP-EIT)\n",
            "35\n",
            "29 Intellectual Property Facilitation Center for entrepreneurs and MSMEs\n",
            "Schemes under SIDBI\n",
            "30 STEP- SIDBI Scheme\n",
            "36\n",
            "31 SWIFT-SIDBI Scheme\n",
            "37\n",
            "32 U\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to apply for PMEGP scheme?\"\n",
        "top_chunks = retrieve_chunks(query, embedded_data, index, tokenizer, model, k=3)\n",
        "\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"\\nüîπ Rank {i+1} (Chunk ID {chunk['chunk_id']}):\")\n",
        "    print(chunk[\"text\"][:400])  # Print first 400 characters of the retrieved chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zqXSCFyAF69",
        "outputId": "4f6549da-fd17-4e3a-db30-e70e2cbb50fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Rank 1 (Chunk ID 79):\n",
            "thoroughly, and make sure that you fall under the \n",
            " \n",
            "eligibility criteria.\n",
            "‚Ä¢ \n",
            "In order to apply, you need to create a Login ID, which would be used in further \n",
            " \n",
            "communications.\n",
            "‚Ä¢ \n",
            "After you have created your user account, you can Login and Apply Online for the Scheme. \n",
            "Before proceeding to apply online, kindly make sure that you have read the Guidelines \n",
            "thoroughly and also checked the List of do\n",
            "\n",
            "üîπ Rank 2 (Chunk ID 3):\n",
            "IC)\n",
            "16-17\n",
            "15 Scheme for Promoting Innovation, Rural Industry & Entrepreneurship (ASPIRE)\n",
            "18\n",
            "16 KHADI GRAMODYOG VIKAS YOJANA - Umbrella Scheme\n",
            "19-20\n",
            "17 Revamped Scheme of Fund for Regeneration of Traditional Industries (SFURTI)\n",
            "21-22\n",
            "18 MSME SUSTAINABLE (ZED) CERTIFICATION\n",
            "23-24\n",
            "19 MSME - Innovative (Incubation, IPR and Design)\n",
            "25-26\n",
            "20 MSME Champions Scheme (Erstwhile CLCS-TUS)\n",
            "27\n",
            "21 Quality Manuf\n",
            "\n",
            "üîπ Rank 3 (Chunk ID 4):\n",
            "PS)\n",
            "32\n",
            "26 Scheme For Promotion of Manufacturing of Electronic Components and \n",
            "Semi-Conductors (Specs)\n",
            "33\n",
            "27 Technology Incubation and Development of Entrepreneurs (TIDE)\n",
            "34\n",
            "28 Support to International Patent Protection in Electronics and IT(SIP-EIT)\n",
            "35\n",
            "29 Intellectual Property Facilitation Center for entrepreneurs and MSMEs\n",
            "Schemes under SIDBI\n",
            "30 STEP- SIDBI Scheme\n",
            "36\n",
            "31 SWIFT-SIDBI Scheme\n",
            "37\n",
            "32 U\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, retrieved_chunks, tokenizer, model, max_tokens=150):\n",
        "    \"\"\"\n",
        "    Generates an answer using TinyLLaMA based on retrieved chunk context.\n",
        "    \"\"\"\n",
        "    # Combine top retrieved chunks into context\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in retrieved_chunks])\n",
        "\n",
        "    # RAG-style prompt\n",
        "    prompt = f\"\"\"Answer the following question based only on the provided context.\n",
        "Be concise and specific.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Tokenize and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False  # use greedy decoding\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer part\n",
        "    return output_text.split(\"Answer:\")[-1].strip()"
      ],
      "metadata": {
        "id": "wbYzyBZ-AJUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Vipplav/tinyllama-finetuned-faq\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")"
      ],
      "metadata": {
        "id": "ZHLTmpyyAitl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and the main causal language model (which will also be used for embeddings)\n",
        "model_name = \"Vipplav/tinyllama-finetuned-faq\"\n",
        "\n",
        "# Load the model once as AutoModelForCausalLM and move it to GPU\n",
        "# This model will be used for both generation and obtaining embeddings.\n",
        "# We assume AutoModelForCausalLM outputs contain hidden states needed for embedding.\n",
        "try:\n",
        "    # Attempt to load with bfloat16 for potential memory savings if GPU supports it\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "except:\n",
        "    # Fallback to default dtype if bfloat16 loading fails\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# Modify the retrieve_chunks function to use the loaded model directly\n",
        "def retrieve_chunks(query, embedded_data, index, tokenizer, model, k=5):\n",
        "    \"\"\"\n",
        "    Encodes query using TinyLLaMA and retrieves top-k similar chunks using FAISS.\n",
        "    \"\"\"\n",
        "    # Tokenize and embed query using the single loaded model\n",
        "    # Ensure output_hidden_states=True is handled if needed by the model type\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        # Pass output_hidden_states=True to get hidden layers from AutoModelForCausalLM\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        # Use the last hidden state for embedding\n",
        "        last_hidden = outputs.hidden_states[-1]\n",
        "        # The mean_pooling function needs the attention mask as well\n",
        "        query_embedding = mean_pooling(last_hidden, inputs[\"attention_mask\"]).cpu().numpy()\n",
        "\n",
        "    # Normalize and search\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Retrieve chunks\n",
        "    results = [embedded_data[i] for i in indices[0]]\n",
        "    return results\n",
        "\n",
        "# Modify the generate_answer function call to use the loaded model directly\n",
        "def generate_answer(query, retrieved_chunks, tokenizer, model, max_tokens=150):\n",
        "    \"\"\"\n",
        "    Generates an answer using TinyLLaMA based on retrieved chunk context.\n",
        "    \"\"\"\n",
        "    # Combine top retrieved chunks into context\n",
        "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in retrieved_chunks])\n",
        "\n",
        "    # RAG-style prompt\n",
        "    prompt = f\"\"\"Answer the following question based only on the provided context.\n",
        "Be concise and specific.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Tokenize and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate using the single loaded model\n",
        "    with torch.no_grad():\n",
        "        # Remove the explicit attention_mask argument as it's already in **inputs\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,  # use greedy decoding\n",
        "            # attention_mask=inputs['attention_mask'] # This line was removed\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer part\n",
        "    return output_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "# Now call the functions using the single 'model' instance\n",
        "query = \"how to apply for ASPIRE scheme?\"\n",
        "\n",
        "# Step 4: Retrieve relevant chunks using the same model (configured as AutoModelForCausalLM)\n",
        "retrieved = retrieve_chunks(query, embedded_data, index, tokenizer, model, k=3)\n",
        "\n",
        "# Step 5: Generate answer using those chunks with the same model\n",
        "answer = generate_answer(query, retrieved, tokenizer, model)\n",
        "\n",
        "print(\"üß† Bot:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9wEq-Z5AtvF",
        "outputId": "9ec92a12-207a-444a-c7b7-e542cc89d1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Bot: For Application, https://wehub.telangana.gov.in/we-engage/\n"
          ]
        }
      ]
    }
  ]
}